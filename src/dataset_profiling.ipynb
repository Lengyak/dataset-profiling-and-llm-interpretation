{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63afb593",
   "metadata": {},
   "source": [
    "# **Justification of the project**\n",
    "\n",
    "The purpose of this Notebook is to generate a structured and comprehensive JSON-based profile of any dataset provided at the path `data/data.csv`, serving as an automated and reusable data profiling stage.\n",
    "\n",
    "The notebook performs a dataset-agnostic exploratory analysis, producing both global metadata and detailed per-column summaries without relying on domain-specific assumptions. At the dataset level, the analysis captures information such as dimensionality, memory usage, duplicated rows, and overall missing-value ratios. At the column level, it extracts data types, cardinality, missing-value statistics, and extended descriptive measures adapted to numerical and categorical variables, while explicitly handling index and identifier columns to prevent their misuse as analytical features.\n",
    "\n",
    "Beyond descriptive profiling, the generated JSON is designed to act as an intermediate artifact for automated reasoning. In a subsequent step, implemented in a separate Python script, this JSON output is used as input to a locally deployed Large Language Model (LLM), for example via Ollama. The LLM is queried to generate a high-level analytical summary of the dataset, extracting conclusions and suggesting potential actions based on the observed statistics and quality indicators.\n",
    "\n",
    "This reasoning step enables the detection and interpretation of relevant patterns that are not explicitly encoded as rules, such as identifying potential outliers when the difference between upper quartiles and maximum values is unusually large, highlighting columns that may require scaling, transformation, or cleaning, and summarizing overall data quality concerns in natural language.\n",
    "\n",
    "The combination of automated statistical profiling and LLM-based interpretative analysis results in a hybrid pipeline that bridges low-level data inspection and high-level analytical insight. The final outcome is a reproducible, extensible workflow that supports data quality assessment, feature engineering decisions, and preliminary analytical conclusions in a form suitable for both technical pipelines and human interpretation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a8fd364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "# Define file paths\n",
    "TEST_DATA_PATH = \"data/data.csv\"\n",
    "OUTPUT_PATH = \"processed_data.json\"\n",
    "\n",
    "# list of strings to be considered as NaN values\n",
    "nan_vals = [\"\", \"NaN\", \"nan\", \"NULL\", \"null\", \"N/A\", \"n/a\", \"NA\", \"na\", \"None\", \"none\", \"-\", \"?\"]\n",
    "# threshold for considering a column as having too many NaN values\n",
    "nan_interval = 0.05 # 5%\n",
    "\n",
    "# read the CSV file into a DataFrame, treating specified strings as NaN\n",
    "df = pd.read_csv(TEST_DATA_PATH, na_values = nan_vals)\n",
    "df.set_index(df.columns[0], inplace=True)\n",
    "\n",
    "# calculate the number of NaN values per column\n",
    "nan_per_column = df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea6c7fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# REPORT INITIAL STRUCTURE\n",
    "# =========================\n",
    "\n",
    "report = {\n",
    "    \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "    \"total_rows\": df.shape[0],\n",
    "    \"total_columns\": df.shape[1],\n",
    "    \"columns\": []\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# DATASET GLOBAL METADATA\n",
    "# =========================\n",
    "\n",
    "report[\"dataset_info\"] = {\n",
    "    \"source_file\": TEST_DATA_PATH,\n",
    "    \"memory_usage_mb\": df.memory_usage(deep=True).sum() / 1e6,\n",
    "    \"duplicated_rows\": int(df.duplicated().sum()),\n",
    "    \"percent_nan_global\": float(\n",
    "        df.isna().sum().sum() / (df.shape[0] * df.shape[1]) * 100\n",
    "    )\n",
    "}\n",
    "\n",
    "report[\"environment\"] = {\n",
    "    \"pandas_version\": pd.__version__,\n",
    "    \"generated_at\": datetime.datetime.now().isoformat()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33d47252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# INDEX REPORT\n",
    "# =========================\n",
    "\n",
    "index_name = df.index.name if df.index.name is not None else \"__index__\"\n",
    "\n",
    "report[\"columns\"].append(\n",
    "    {\n",
    "        \"column_name\": index_name,\n",
    "        \"data_type\": str(df.index.dtype),\n",
    "        \"num_unique_values\": int(pd.Index(df.index).nunique()),\n",
    "        \"num_nan_values\": int(pd.isna(df.index).sum()),\n",
    "        \"percent_nan\": (pd.isna(df.index).sum() / df.shape[0]) * 100,\n",
    "        \"is_index\": True,\n",
    "        \"role\": \"index\",\n",
    "        \"recommendation\": \"keep\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e5a0c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tonac\\AppData\\Local\\Temp\\ipykernel_20292\\3262591538.py:60: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  elif pd.api.types.is_categorical_dtype(col) or pd.api.types.is_object_dtype(col):\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# COLUMN PROFILING\n",
    "# =========================\n",
    "\n",
    "for column in df.columns:\n",
    "\n",
    "    col = df[column]\n",
    "    num_nan = int(col.isna().sum())\n",
    "    percent_nan = (num_nan / df.shape[0]) * 100\n",
    "    num_unique = int(col.nunique(dropna=True))\n",
    "\n",
    "    col_report = {\n",
    "        \"column_name\": column,\n",
    "        \"data_type\": str(col.dtype),\n",
    "        \"num_unique_values\": num_unique,\n",
    "        \"num_nan_values\": num_nan,\n",
    "        \"percent_nan\": percent_nan,\n",
    "        \"quality_warnings\": []\n",
    "    }\n",
    "\n",
    "    # =========================\n",
    "    # SEMANTIC FLAGS\n",
    "    # =========================\n",
    "\n",
    "    col_report[\"is_constant\"] = num_unique == 1\n",
    "    col_report[\"is_sparse\"] = percent_nan > (nan_interval * 100)\n",
    "    col_report[\"is_identifier\"] = (num_unique / df.shape[0]) > 0.95 and not pd.api.types.is_float_dtype(col)\n",
    "    col_report[\"role\"] = \"feature\"\n",
    "\n",
    "    if col_report[\"is_identifier\"]:\n",
    "        col_report[\"role\"] = \"identifier\"\n",
    "\n",
    "    # =========================\n",
    "    # NUMERIC FEATURES\n",
    "    # =========================\n",
    "\n",
    "    if pd.api.types.is_numeric_dtype(col):\n",
    "\n",
    "        q25 = col.quantile(0.25)\n",
    "        q75 = col.quantile(0.75)\n",
    "\n",
    "        col_report.update({\n",
    "            \"min_value\": col.min(),\n",
    "            \"mean\": col.mean(),\n",
    "            \"std\": col.std(),\n",
    "            \"25_quartile\": q25,\n",
    "            \"median\": col.median(),\n",
    "            \"75_quartile\": q75,\n",
    "            \"max_value\": col.max(),\n",
    "            \"iqr\": q75 - q25\n",
    "        })\n",
    "\n",
    "        if col_report[\"std\"] == 0 or pd.isna(col_report[\"std\"]):\n",
    "            col_report[\"quality_warnings\"].append(\"zero_variance\")\n",
    "\n",
    "    # =========================\n",
    "    # CATEGORICAL FEATURES\n",
    "    # =========================\n",
    "\n",
    "    elif pd.api.types.is_categorical_dtype(col) or pd.api.types.is_object_dtype(col):\n",
    "\n",
    "        value_counts = col.value_counts(dropna=True)\n",
    "\n",
    "        if not value_counts.empty:\n",
    "            col_report[\"most_common\"] = value_counts.index[0]\n",
    "            col_report[\"most_common_freq\"] = int(value_counts.iloc[0])\n",
    "            col_report[\"top_3_values\"] = value_counts.head(3).to_dict()\n",
    "            col_report[\"num_rare_categories\"] = int((value_counts == 1).sum())\n",
    "\n",
    "    # =========================\n",
    "    # QUALITY WARNINGS\n",
    "    # =========================\n",
    "\n",
    "    if percent_nan > 50:\n",
    "        col_report[\"quality_warnings\"].append(\"high_missing_ratio\")\n",
    "\n",
    "    if col_report[\"is_constant\"]:\n",
    "        col_report[\"quality_warnings\"].append(\"constant_column\")\n",
    "\n",
    "    # =========================\n",
    "    # RECOMMENDATIONS\n",
    "    # =========================\n",
    "\n",
    "    if col_report[\"is_identifier\"]:\n",
    "        col_report[\"recommendation\"] = \"exclude_from_model\"\n",
    "    elif percent_nan > 50:\n",
    "        col_report[\"recommendation\"] = \"consider_drop\"\n",
    "    elif pd.api.types.is_numeric_dtype(col):\n",
    "        col_report[\"recommendation\"] = \"consider_scaling\"\n",
    "    else:\n",
    "        col_report[\"recommendation\"] = \"consider_encoding\"\n",
    "\n",
    "    report[\"columns\"].append(col_report)\n",
    "\n",
    "# =========================\n",
    "# GLOBAL QUALITY SUMMARY\n",
    "# =========================\n",
    "\n",
    "report[\"quality_summary\"] = {\n",
    "    \"columns_high_nan\": [\n",
    "        c[\"column_name\"]\n",
    "        for c in report[\"columns\"]\n",
    "        if c.get(\"percent_nan\", 0) > 50\n",
    "    ],\n",
    "    \"constant_columns\": [\n",
    "        c[\"column_name\"]\n",
    "        for c in report[\"columns\"]\n",
    "        if c.get(\"is_constant\")\n",
    "    ],\n",
    "    \"identifier_columns\": [\n",
    "        c[\"column_name\"]\n",
    "        for c in report[\"columns\"]\n",
    "        if c.get(\"is_identifier\")\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72f4af40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# JSON EXPORT\n",
    "# =========================\n",
    "\n",
    "report_json = json.dumps(report, indent=4)\n",
    "\n",
    "with open(OUTPUT_PATH, \"w\") as f:\n",
    "    f.write(report_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia_confiable",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
